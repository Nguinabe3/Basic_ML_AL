# _Author : NGUINABE JOSUE_

### Date created

 **12 - 03 - 2024**

#### This is coding project as part of AMMI program. This project content the implementation from scratch of the follwing algorithms :

> Linear Regression : Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables and is often used for prediction and forecasting tasks.

> Logistic Regression : Logistic regression is a classification algorithm used to predict the probability of a binary outcome (0 or 1, true or false). It models the relationship between the dependent variable and one or more independent variables by using the logistic function to squash the output into a probability between 0 and 1.

> Basch Gradient Descent : Batch gradient descent is an optimization algorithm used to minimize the cost function in machine learning models. It calculates the gradient of the cost function with respect to the parameters using the entire training dataset in each iteration, which can be computationally expensive for large datasets.

> Stochastic Gradient Descent : Stochastic gradient descent is a variant of gradient descent where the parameters are updated using a single randomly selected sample from the training dataset in each iteration. This makes it computationally more efficient than batch gradient descent, especially for large datasets.

> Stochastic Gradient Descent with momentum : SGD with momentum is an extension of stochastic gradient descent that introduces a momentum term to accelerate convergence and dampen oscillations in the parameter updates. It helps the optimization algorithm to move faster in relevant directions and smoothes out updates in irrelevant directions.

> Mini-Basch Gradient Descent : Mini-batch gradient descent is a compromise between batch gradient descent and stochastic gradient descent. It divides the training dataset into small batches and updates the parameters using each batch in each iteration. This approach combines the efficiency of stochastic gradient descent with the stability of batch gradient descent. The batch size is typically chosen based on computational resources and convergence considerations.

**Please, feel free to test it and give me some feedback in order to help me learn more and improve my coding skill.**


## Installation:

1. Make sure you have Python installed on your machine.
2. Clone this repository using the following command: 
   git clone <repository_url>

## Usage:

1. Navigate to the directory of the specific folder.
2. Run the Python script named main.py
3. After running, just follow the intruction because the code is intercative as most as possiple to guide you with the comment on each function.
4. If you have you own dataset, just change the the name of the current data with your dataset


## License:

[MIT](https://choosealicense.com/licenses/mit/)
   


%%%%%%%%%%** Looking forward to hearing from you soon **%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%**Best Regards**%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%_JOSUE NGUINABE_%%%%%%%%%%%%%%%%%%%%%%%%%

